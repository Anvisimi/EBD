{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b4146-934d-42f3-8f0c-bb301eeea701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeStreamingData\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC Configs\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Read Tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "machinestatus_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarmcodes_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# Step 1: Get best MES within ±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff_mes\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff_mes\") <= 3600)\n",
    "\n",
    "window_mes = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff_mes\")\n",
    "best_mes = mes_join.withColumn(\"rn\", row_number().over(window_mes)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# Step 2: Enrich SCADA with StatusID and AlarmID\n",
    "scada_enriched = scada_df.alias(\"s\") \\\n",
    "    .join(machinestatus_df.alias(\"ms\"), col(\"s.Machine_Status\") == col(\"ms.StatusName\"), \"left\") \\\n",
    "    .join(alarmcodes_df.alias(\"ac\"), col(\"s.Alarm_Code\") == col(\"ac.AlarmDescription\"), \"left\") \\\n",
    "    .select(\n",
    "        col(\"s.Timestamp\").alias(\"scada_ts\"),\n",
    "        col(\"s.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "        col(\"s.Machine_Status\"),\n",
    "        col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "        col(\"s.Alarm_Code\")\n",
    "    )\n",
    "\n",
    "# Step 3: Get best SCADA within ±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_enriched.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff_scada\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.scada_ts\")))) \\\n",
    "    .filter(col(\"time_diff_scada\") <= 900)\n",
    "\n",
    "window_scada = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff_scada\")\n",
    "best_scada = scada_join.withColumn(\"rn\", row_number().over(window_scada)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status_ID\"),\n",
    "        col(\"s.Machine_Status\"),\n",
    "        col(\"s.Alarm_ID\"),\n",
    "        col(\"s.Alarm_Code\")\n",
    "    )\n",
    "\n",
    "# Step 4: Final Join MES + SCADA\n",
    "final_df = best_mes.alias(\"m\").join(best_scada.alias(\"s\"), [\"Machine_ID\", \"Timestamp\"], \"left\") \\\n",
    "    .select(\n",
    "        col(\"Timestamp\"),\n",
    "        col(\"Machine_ID\"),\n",
    "        col(\"Temperature_C\"),\n",
    "        col(\"Vibration_mm_s\"),\n",
    "        col(\"Pressure_bar\"),\n",
    "        col(\"Operator_ID\"),\n",
    "        col(\"Units_Produced\"),\n",
    "        col(\"Defective_Units\"),\n",
    "        col(\"Production_Time_min\"),\n",
    "        col(\"Power_Consumption_kW\"),\n",
    "        col(\"Machine_Status_ID\"),\n",
    "        col(\"Machine_Status\"),\n",
    "        col(\"Alarm_ID\"),\n",
    "        col(\"Alarm_Code\")\n",
    "    )\n",
    "\n",
    "# Optional: reduce to fewer partitions\n",
    "final_df = final_df.repartition(1)\n",
    "\n",
    "# Step 5: Write to machine_metrics\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"✅ Merge complete and data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b764308-0a3e-4adf-a27e-f5bee377d32c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    " \n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeStreamingData\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    " \n",
    "# JDBC Configs\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    " \n",
    "# Read Tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "machinestatus_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarmcodes_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    " \n",
    "# Step 1: Get best MES within ±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff_mes\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff_mes\") <= 3600)\n",
    " \n",
    "window_mes = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff_mes\")\n",
    "best_mes = mes_join.withColumn(\"rn\", row_number().over(window_mes)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    " \n",
    "# Step 2: Enrich SCADA with StatusID and AlarmID\n",
    "scada_enriched = scada_df.alias(\"s\") \\\n",
    "    .join(machinestatus_df.alias(\"ms\"), col(\"s.Machine_Status\") == col(\"ms.StatusName\"), \"left\") \\\n",
    "    .join(alarmcodes_df.alias(\"ac\"), col(\"s.Alarm_Code\") == col(\"ac.AlarmDescription\"), \"left\") \\\n",
    "    .select(\n",
    "        col(\"s.Timestamp\").alias(\"scada_ts\"),\n",
    "        col(\"s.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "        col(\"s.Machine_Status\"),\n",
    "        col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "        col(\"s.Alarm_Code\")\n",
    "    )\n",
    " \n",
    "# Step 3: Get best SCADA within ±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_enriched.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff_scada\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.scada_ts\")))) \\\n",
    "    .filter(col(\"time_diff_scada\") <= 900)\n",
    " \n",
    "window_scada = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff_scada\")\n",
    "best_scada = scada_join.withColumn(\"rn\", row_number().over(window_scada)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status_ID\"),\n",
    "        col(\"s.Machine_Status\"),\n",
    "        col(\"s.Alarm_ID\"),\n",
    "        col(\"s.Alarm_Code\")\n",
    "    )\n",
    " \n",
    "# Step 4: Final Join MES + SCADA\n",
    "final_df = best_mes.alias(\"m\").join(best_scada.alias(\"s\"), [\"Machine_ID\", \"Timestamp\"], \"left\") \\\n",
    "    .select(\n",
    "        col(\"Timestamp\"),\n",
    "        col(\"Machine_ID\"),\n",
    "        col(\"Temperature_C\"),\n",
    "        col(\"Vibration_mm_s\"),\n",
    "        col(\"Pressure_bar\"),\n",
    "        col(\"Operator_ID\"),\n",
    "        col(\"Units_Produced\"),\n",
    "        col(\"Defective_Units\"),\n",
    "        col(\"Production_Time_min\"),\n",
    "        col(\"Power_Consumption_kW\"),\n",
    "        col(\"Machine_Status_ID\"),\n",
    "        col(\"Machine_Status\"),\n",
    "        col(\"Alarm_ID\"),\n",
    "        col(\"Alarm_Code\")\n",
    "    )\n",
    " \n",
    "# Optional: reduce to fewer partitions\n",
    "final_df = final_df.repartition(1)\n",
    " \n",
    "# Step 5: Write to machine_metrics\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics_test\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"✅ Merge complete and data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3203f-59f1-4472-a9be-6b5724a2e02c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number, trim, lower\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeStreamingData\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configuration\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Read all tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "machinestatus_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarmcodes_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# Best MES within ±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff_mes\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff_mes\") <= 3600)\n",
    "\n",
    "window_mes = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff_mes\")\n",
    "best_mes = mes_join.withColumn(\"rn\", row_number().over(window_mes)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# Best SCADA within ±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff_scada\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff_scada\") <= 900)\n",
    "\n",
    "window_scada = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff_scada\")\n",
    "best_scada = scada_join.withColumn(\"rn\", row_number().over(window_scada)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\"),\n",
    "        col(\"s.Alarm_Code\")\n",
    "    )\n",
    "\n",
    "# Enrich SCADA with Machine_Status_ID and Alarm_ID\n",
    "scada_enriched = best_scada.alias(\"s\") \\\n",
    "    .join(machinestatus_df.alias(\"ms\"), trim(lower(col(\"s.Machine_Status\"))) == trim(lower(col(\"ms.StatusName\"))), \"left\") \\\n",
    "    .join(alarmcodes_df.alias(\"ac\"), trim(lower(col(\"s.Alarm_Code\"))) == trim(lower(col(\"ac.AlarmDescription\"))), \"left\") \\\n",
    "    .select(\n",
    "        col(\"s.Timestamp\"),\n",
    "        col(\"s.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "        col(\"s.Machine_Status\"),\n",
    "        col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "        col(\"s.Alarm_Code\")\n",
    "    )\n",
    "\n",
    "# Final join\n",
    "final_df = best_mes.alias(\"m\").join(scada_enriched.alias(\"s\"), [\"Machine_ID\", \"Timestamp\"], \"left\") \\\n",
    "    .select(\n",
    "        col(\"Timestamp\"),\n",
    "        col(\"Machine_ID\"),\n",
    "        col(\"Temperature_C\"),\n",
    "        col(\"Vibration_mm_s\"),\n",
    "        col(\"Pressure_bar\"),\n",
    "        col(\"Operator_ID\"),\n",
    "        col(\"Units_Produced\"),\n",
    "        col(\"Defective_Units\"),\n",
    "        col(\"Production_Time_min\"),\n",
    "        col(\"Power_Consumption_kW\"),\n",
    "        col(\"Machine_Status_ID\"),\n",
    "        col(\"Machine_Status\"),\n",
    "        col(\"Alarm_ID\"),\n",
    "        col(\"Alarm_Code\")\n",
    "    )\n",
    "\n",
    "# Optional: Reduce Spark write concurrency to avoid StarRocks tablet version overflow\n",
    "final_df = final_df.repartition(1)\n",
    "\n",
    "# Write to StarRocks\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"✅ Merge complete and data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651d8f5-2ef0-4515-a32b-04c529ea1d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number, lower, trim\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeStreamingData\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configurations\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Read staging tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "machinestatus_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarmcodes_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# MES join\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"diff\", abs(unix_timestamp(\"i.Timestamp\") - unix_timestamp(\"m.Timestamp\"))) \\\n",
    "    .filter(col(\"diff\") <= 3600)\n",
    "\n",
    "mes_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"diff\")\n",
    "best_mes = mes_join.withColumn(\"rn\", row_number().over(mes_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# SCADA join\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"diff\", abs(unix_timestamp(\"i.Timestamp\") - unix_timestamp(\"s.Timestamp\"))) \\\n",
    "    .filter(col(\"diff\") <= 900)\n",
    "\n",
    "scada_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"diff\")\n",
    "best_scada = scada_join.withColumn(\"rn\", row_number().over(scada_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\"),\n",
    "        col(\"s.Alarm_Code\")\n",
    "    )\n",
    "\n",
    "# FIXED: Join using trimmed/lowercased text to get proper ID matches\n",
    "scada_enriched = best_scada.alias(\"s\") \\\n",
    "    .join(machinestatus_df.alias(\"ms\"), trim(lower(col(\"s.Machine_Status\"))) == trim(lower(col(\"ms.StatusName\"))), \"left\") \\\n",
    "    .join(alarmcodes_df.alias(\"ac\"), trim(lower(col(\"s.Alarm_Code\"))) == trim(lower(col(\"ac.AlarmDescription\"))), \"left\") \\\n",
    "    .select(\n",
    "        \"s.Timestamp\", \"s.Machine_ID\", \"Power_Consumption_kW\",\n",
    "        col(\"ms.StatusID\").alias(\"Machine_Status_ID\"), \"s.Machine_Status\",\n",
    "        col(\"ac.AlarmID\").alias(\"Alarm_ID\"), \"s.Alarm_Code\"\n",
    "    )\n",
    "\n",
    "# Final merge\n",
    "final_df = best_mes.alias(\"m\").join(scada_enriched.alias(\"s\"),\n",
    "    (col(\"m.Machine_ID\") == col(\"s.Machine_ID\")) & (col(\"m.Timestamp\") == col(\"s.Timestamp\")),\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    col(\"m.Timestamp\"), col(\"m.Machine_ID\"),\n",
    "    \"Temperature_C\", \"Vibration_mm_s\", \"Pressure_bar\",\n",
    "    \"Operator_ID\", \"Units_Produced\", \"Defective_Units\", \"Production_Time_min\",\n",
    "    \"Power_Consumption_kW\", \"Machine_Status_ID\", \"Machine_Status\",\n",
    "    \"Alarm_ID\", \"Alarm_Code\"\n",
    ")\n",
    "\n",
    "# Show or write to table\n",
    "# final_df.show()\n",
    "\n",
    "final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"machine_metrics\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dda882-8016-48a0-b4bf-f963e9fb7c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeStreamingData\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configurations\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Load tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "machinestatus_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarmcodes_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# MES join\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"diff\", abs(unix_timestamp(\"i.Timestamp\") - unix_timestamp(\"m.Timestamp\"))) \\\n",
    "    .filter(col(\"diff\") <= 3600)\n",
    "\n",
    "mes_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"diff\")\n",
    "best_mes = mes_join.withColumn(\"rn\", row_number().over(mes_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# SCADA join\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"diff\", abs(unix_timestamp(\"i.Timestamp\") - unix_timestamp(\"s.Timestamp\"))) \\\n",
    "    .filter(col(\"diff\") <= 900)\n",
    "\n",
    "scada_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"diff\")\n",
    "best_scada = scada_join.withColumn(\"rn\", row_number().over(scada_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\"),\n",
    "        col(\"s.Alarm_Code\")\n",
    "    )\n",
    "\n",
    "# Enrich with status and alarm IDs\n",
    "scada_enriched = best_scada.alias(\"s\") \\\n",
    "    .join(machinestatus_df.alias(\"ms\"), col(\"s.Machine_Status\") == col(\"ms.StatusName\"), \"left\") \\\n",
    "    .join(alarmcodes_df.alias(\"ac\"), col(\"s.Alarm_Code\") == col(\"ac.AlarmDescription\"), \"left\") \\\n",
    "    .select(\n",
    "        \"s.Timestamp\", \"s.Machine_ID\", \"Power_Consumption_kW\",\n",
    "        col(\"ms.StatusID\").alias(\"Machine_Status_ID\"), \"s.Machine_Status\",\n",
    "        col(\"ac.AlarmID\").alias(\"Alarm_ID\"), \"s.Alarm_Code\"\n",
    "    )\n",
    "\n",
    "# Final merge\n",
    "final_df = best_mes.alias(\"m\").join(scada_enriched.alias(\"s\"),\n",
    "    (col(\"m.Machine_ID\") == col(\"s.Machine_ID\")) & (col(\"m.Timestamp\") == col(\"s.Timestamp\")),\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    col(\"m.Timestamp\"), col(\"m.Machine_ID\"),\n",
    "    \"Temperature_C\", \"Vibration_mm_s\", \"Pressure_bar\",\n",
    "    \"Operator_ID\", \"Units_Produced\", \"Defective_Units\", \"Production_Time_min\",\n",
    "    \"Power_Consumption_kW\", \"Machine_Status_ID\", \"Machine_Status\",\n",
    "    \"Alarm_ID\", \"Alarm_Code\"\n",
    ")\n",
    "\n",
    "final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"machine_metrics\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fed9e5-e376-4aa1-9222-8fab225e3412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeToMachineMetrics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configs\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Load staging and dimension tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "status_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarm_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# Best MES within ±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 3600)\n",
    "\n",
    "mes_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "mes_best = mes_join.withColumn(\"rn\", row_number().over(mes_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# Best SCADA within ±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 900)\n",
    "\n",
    "scada_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "scada_best = scada_join.withColumn(\"rn\", row_number().over(scada_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\").alias(\"Machine_Status_Name\"),\n",
    "        col(\"s.Alarm_Code\").alias(\"Alarm_Code_Desc\")\n",
    "    )\n",
    "\n",
    "# Join all and enrich with IDs\n",
    "final_df = mes_best.alias(\"m\").join(scada_best.alias(\"s\"),\n",
    "    (col(\"m.Machine_ID\") == col(\"s.Machine_ID\")) & (col(\"m.iot_Timestamp\") == col(\"s.iot_Timestamp\")),\n",
    "    how=\"left\"\n",
    ").join(status_df.alias(\"ms\"), col(\"s.Machine_Status_Name\") == col(\"ms.StatusName\"), \"left\") \\\n",
    " .join(alarm_df.alias(\"ac\"), col(\"s.Alarm_Code_Desc\") == col(\"ac.AlarmDescription\"), \"left\") \\\n",
    " .select(\n",
    "    col(\"m.iot_Timestamp\").alias(\"Timestamp\"),\n",
    "    col(\"m.Machine_ID\"),\n",
    "    col(\"m.Temperature_C\"),\n",
    "    col(\"m.Vibration_mm_s\"),\n",
    "    col(\"m.Pressure_bar\"),\n",
    "    col(\"m.Operator_ID\"),\n",
    "    col(\"m.Units_Produced\"),\n",
    "    col(\"m.Defective_Units\"),\n",
    "    col(\"m.Production_Time_min\"),\n",
    "    col(\"s.Power_Consumption_kW\"),\n",
    "    col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "    col(\"ms.StatusName\").alias(\"Machine_Status\"),\n",
    "    col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "    col(\"ac.AlarmDescription\").alias(\"Alarm_Code\")\n",
    ")\n",
    "\n",
    "# Optional: Write to StarRocks\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"✅ Data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568e45e-edca-4acc-9ca9-d2289660c10d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeToMachineMetrics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configs\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Load staging and dimension tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "status_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarm_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# Best MES within ±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 3600)\n",
    "\n",
    "mes_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "mes_best = mes_join.withColumn(\"rn\", row_number().over(mes_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# Best SCADA within ±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 900)\n",
    "\n",
    "scada_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "scada_best = scada_join.withColumn(\"rn\", row_number().over(scada_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\").alias(\"Machine_Status_Name\"),\n",
    "        col(\"s.Alarm_Code\").alias(\"Alarm_Code_Desc\")\n",
    "    )\n",
    "\n",
    "# Join all and enrich with IDs\n",
    "final_df = mes_best.alias(\"m\").join(scada_best.alias(\"s\"),\n",
    "    (col(\"m.Machine_ID\") == col(\"s.Machine_ID\")) & (col(\"m.iot_Timestamp\") == col(\"s.iot_Timestamp\")),\n",
    "    how=\"left\"\n",
    ").join(status_df.alias(\"ms\"), col(\"s.Machine_Status_Name\") == col(\"ms.StatusName\"), \"left\") \\\n",
    " .join(alarm_df.alias(\"ac\"), col(\"s.Alarm_Code_Desc\") == col(\"ac.AlarmDescription\"), \"left\") \\\n",
    " .select(\n",
    "    col(\"m.iot_Timestamp\").alias(\"Timestamp\"),\n",
    "    col(\"m.Machine_ID\"),\n",
    "    col(\"m.Temperature_C\"),\n",
    "    col(\"m.Vibration_mm_s\"),\n",
    "    col(\"m.Pressure_bar\"),\n",
    "    col(\"m.Operator_ID\"),\n",
    "    col(\"m.Units_Produced\"),\n",
    "    col(\"m.Defective_Units\"),\n",
    "    col(\"m.Production_Time_min\"),\n",
    "    col(\"s.Power_Consumption_kW\"),\n",
    "    col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "    col(\"ms.StatusName\").alias(\"Machine_Status\"),\n",
    "    col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "    col(\"ac.AlarmDescription\").alias(\"Alarm_Code\")\n",
    ")\n",
    "\n",
    "# Optional: Write to StarRocks\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"✅ Data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d68bf7-29c6-4f2c-bdce-b007dde66eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeToMachineMetrics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configs\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Load staging and dimension tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "status_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarm_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# Best MES within ±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 3600)\n",
    "\n",
    "mes_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "mes_best = mes_join.withColumn(\"rn\", row_number().over(mes_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# Best SCADA within ±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 900)\n",
    "\n",
    "scada_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "scada_best = scada_join.withColumn(\"rn\", row_number().over(scada_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\").alias(\"Machine_Status_Name\"),\n",
    "        col(\"s.Alarm_Code\").alias(\"Alarm_Code_Desc\")\n",
    "    )\n",
    "\n",
    "# Join all and enrich with IDs\n",
    "final_df = mes_best.alias(\"m\").join(scada_best.alias(\"s\"),\n",
    "    (col(\"m.Machine_ID\") == col(\"s.Machine_ID\")) & (col(\"m.iot_Timestamp\") == col(\"s.iot_Timestamp\")),\n",
    "    how=\"left\"\n",
    ").join(status_df.alias(\"ms\"), col(\"s.Machine_Status_Name\") == col(\"ms.StatusName\"), \"left\") \\\n",
    " .join(alarm_df.alias(\"ac\"),\n",
    "      col(\"s.Alarm_Code_Desc\").isNull() & (col(\"ac.AlarmDescription\") == \"None\") |\n",
    "      (col(\"s.Alarm_Code_Desc\") == col(\"ac.AlarmDescription\")),\n",
    "      \"left\").select(\n",
    "    col(\"m.iot_Timestamp\").alias(\"Timestamp\"),\n",
    "    col(\"m.Machine_ID\"),\n",
    "    col(\"m.Temperature_C\"),\n",
    "    col(\"m.Vibration_mm_s\"),\n",
    "    col(\"m.Pressure_bar\"),\n",
    "    col(\"m.Operator_ID\"),\n",
    "    col(\"m.Units_Produced\"),\n",
    "    col(\"m.Defective_Units\"),\n",
    "    col(\"m.Production_Time_min\"),\n",
    "    col(\"s.Power_Consumption_kW\"),\n",
    "    col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "    col(\"ms.StatusName\").alias(\"Machine_Status\"),\n",
    "    col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "    col(\"ac.AlarmDescription\").alias(\"Alarm_Code\")\n",
    ")\n",
    "\n",
    "# Optional: Write to StarRocks\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"✅ Data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ddd30-d6a3-40e4-a268-3b028a7a8956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeToMachineMetrics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configs\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Load staging and dimension tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "status_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarm_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# Best MES within ±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 3600)\n",
    "\n",
    "mes_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "mes_best = mes_join.withColumn(\"rn\", row_number().over(mes_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# Best SCADA within ±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 900)\n",
    "\n",
    "scada_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "scada_best = scada_join.withColumn(\"rn\", row_number().over(scada_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\").alias(\"Machine_Status_Name\"),\n",
    "        col(\"s.Alarm_Code\").alias(\"Alarm_Code_Desc\")\n",
    "    )\n",
    "scada_best = scada_best.fillna({\"Alarm_Code_Desc\": \"None\"})\n",
    "\n",
    "# Join all and enrich with IDs\n",
    "final_df = mes_best.alias(\"m\").join(scada_best.alias(\"s\"),\n",
    "    (col(\"m.Machine_ID\") == col(\"s.Machine_ID\")) & (col(\"m.iot_Timestamp\") == col(\"s.iot_Timestamp\")),\n",
    "    how=\"left\"\n",
    ").join(status_df.alias(\"ms\"), col(\"s.Machine_Status_Name\") == col(\"ms.StatusName\"), \"left\") \\\n",
    " .join(alarm_df.alias(\"ac\"), col(\"s.Alarm_Code_Desc\") == col(\"ac.AlarmDescription\"), \"left\").select(\n",
    "    col(\"m.iot_Timestamp\").alias(\"Timestamp\"),\n",
    "    col(\"m.Machine_ID\"),\n",
    "    col(\"m.Temperature_C\"),\n",
    "    col(\"m.Vibration_mm_s\"),\n",
    "    col(\"m.Pressure_bar\"),\n",
    "    col(\"m.Operator_ID\"),\n",
    "    col(\"m.Units_Produced\"),\n",
    "    col(\"m.Defective_Units\"),\n",
    "    col(\"m.Production_Time_min\"),\n",
    "    col(\"s.Power_Consumption_kW\"),\n",
    "    col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "    col(\"ms.StatusName\").alias(\"Machine_Status\"),\n",
    "    col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "    col(\"ac.AlarmDescription\").alias(\"Alarm_Code\")\n",
    ")\n",
    "\n",
    "# Optional: Write to StarRocks\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"✅ Data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec238d73-6cc2-49f3-b3bb-cdb223cba831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number, when, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeToMachineMetrics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configs\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Load staging and dimension tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "status_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarm_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# Best MES within ±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 3600)\n",
    "\n",
    "mes_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "mes_best = mes_join.withColumn(\"rn\", row_number().over(mes_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# Best SCADA within ±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 900)\n",
    "\n",
    "scada_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "scada_best = scada_join.withColumn(\"rn\", row_number().over(scada_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\").alias(\"Machine_Status_Name\"),\n",
    "        col(\"s.Alarm_Code\").alias(\"Alarm_Code_Desc\")\n",
    "    )\n",
    "scada_best = scada_best.fillna({\"Alarm_Code_Desc\": \"None\"})\n",
    "scada_best = scada_best.withColumn(\n",
    "    \"Alarm_Code_Desc\",\n",
    "    when(col(\"Alarm_Code_Desc\").isNull() | (col(\"Alarm_Code_Desc\") == \"\"), lit(\"None\"))\n",
    "    .otherwise(col(\"Alarm_Code_Desc\"))\n",
    ")\n",
    "\n",
    "# Join all and enrich with IDs\n",
    "final_df = mes_best.alias(\"m\").join(scada_best.alias(\"s\"),\n",
    "    (col(\"m.Machine_ID\") == col(\"s.Machine_ID\")) & (col(\"m.iot_Timestamp\") == col(\"s.iot_Timestamp\")),\n",
    "    how=\"left\"\n",
    ").join(status_df.alias(\"ms\"), col(\"s.Machine_Status_Name\") == col(\"ms.StatusName\"), \"left\") \\\n",
    " .join(alarm_df.alias(\"ac\"), col(\"s.Alarm_Code_Desc\") == col(\"ac.AlarmDescription\"), \"left\").select(\n",
    "    col(\"m.iot_Timestamp\").alias(\"Timestamp\"),\n",
    "    col(\"m.Machine_ID\"),\n",
    "    col(\"m.Temperature_C\"),\n",
    "    col(\"m.Vibration_mm_s\"),\n",
    "    col(\"m.Pressure_bar\"),\n",
    "    col(\"m.Operator_ID\"),\n",
    "    col(\"m.Units_Produced\"),\n",
    "    col(\"m.Defective_Units\"),\n",
    "    col(\"m.Production_Time_min\"),\n",
    "    col(\"s.Power_Consumption_kW\"),\n",
    "    col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "    col(\"ms.StatusName\").alias(\"Machine_Status\"),\n",
    "    col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "    col(\"ac.AlarmDescription\").alias(\"Alarm_Code\")\n",
    ")\n",
    "\n",
    "# Optional: Write to StarRocks\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"✅ Data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194ae31-4942-4f1d-8f95-aae5c0037719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeToMachineMetrics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configs\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Load staging and dimension tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "status_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarm_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# Best MES within ±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 3600)\n",
    "\n",
    "mes_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "mes_best = mes_join.withColumn(\"rn\", row_number().over(mes_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# Best SCADA within ±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 900)\n",
    "\n",
    "scada_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "scada_best = scada_join.withColumn(\"rn\", row_number().over(scada_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\").alias(\"Machine_Status_Name\"),\n",
    "        col(\"s.Alarm_Code\").alias(\"Alarm_Code_Desc\")\n",
    "    )\n",
    "\n",
    "# Join all and enrich with IDs\n",
    "final_df = mes_best.alias(\"m\").join(scada_best.alias(\"s\"),\n",
    "    (col(\"m.Machine_ID\") == col(\"s.Machine_ID\")) & (col(\"m.iot_Timestamp\") == col(\"s.iot_Timestamp\")),\n",
    "    how=\"left\"\n",
    ").join(status_df.alias(\"ms\"), col(\"s.Machine_Status_Name\") == col(\"ms.StatusName\"), \"left\") \\\n",
    " .join(alarm_df.alias(\"ac\"), col(\"s.Alarm_Code_Desc\") == col(\"ac.AlarmDescription\"), \"left\") \\\n",
    " .select(\n",
    "    col(\"m.iot_Timestamp\").alias(\"Timestamp\"),\n",
    "    col(\"m.Machine_ID\"),\n",
    "    col(\"m.Temperature_C\"),\n",
    "    col(\"m.Vibration_mm_s\"),\n",
    "    col(\"m.Pressure_bar\"),\n",
    "    col(\"m.Operator_ID\"),\n",
    "    col(\"m.Units_Produced\"),\n",
    "    col(\"m.Defective_Units\"),\n",
    "    col(\"m.Production_Time_min\"),\n",
    "    col(\"s.Power_Consumption_kW\"),\n",
    "    col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "    col(\"ms.StatusName\").alias(\"Machine_Status\"),\n",
    "    col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "    col(\"ac.AlarmDescription\").alias(\"Alarm_Code\")\n",
    ")\n",
    "\n",
    "# Optional: Write to StarRocks\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"✅ Data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6dd49-ac16-41c1-a16c-99472b161e68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, when\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeToMachineMetrics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configs\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Load staging and dimension tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "status_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarm_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# Best MES within ±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 3600)\n",
    "\n",
    "mes_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "mes_best = mes_join.withColumn(\"rn\", row_number().over(mes_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# Best SCADA within ±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 900)\n",
    "\n",
    "scada_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "scada_best = scada_join.withColumn(\"rn\", row_number().over(scada_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\").alias(\"Machine_Status_Name\"),\n",
    "        col(\"s.Alarm_Code\").alias(\"Alarm_Code_Desc\")\n",
    "    )\n",
    "scada_best = scada_best.fillna({\"Alarm_Code_Desc\": \"None\"})\n",
    "\n",
    "scada_best = scada_best.withColumn(\n",
    "    \"Alarm_Code_Desc\",\n",
    "    when(col(\"Alarm_Code_Desc\").isNull() | (col(\"Alarm_Code_Desc\") == \"\"), lit(\"None\"))\n",
    "    .otherwise(col(\"Alarm_Code_Desc\"))\n",
    ")\n",
    "\n",
    "# Join all and enrich with IDs\n",
    "final_df = mes_best.alias(\"m\").join(scada_best.alias(\"s\"),\n",
    "    (col(\"m.Machine_ID\") == col(\"s.Machine_ID\")) & (col(\"m.iot_Timestamp\") == col(\"s.iot_Timestamp\")),\n",
    "    how=\"left\"\n",
    ").join(status_df.alias(\"ms\"), col(\"s.Machine_Status_Name\") == col(\"ms.StatusName\"), \"left\") \\\n",
    " .join(alarm_df.alias(\"ac\"), col(\"s.Alarm_Code_Desc\") == col(\"ac.AlarmDescription\"), \"left\") \\\n",
    " .select(\n",
    "    col(\"m.iot_Timestamp\").alias(\"Timestamp\"),\n",
    "    col(\"m.Machine_ID\"),\n",
    "    col(\"m.Temperature_C\"),\n",
    "    col(\"m.Vibration_mm_s\"),\n",
    "    col(\"m.Pressure_bar\"),\n",
    "    col(\"m.Operator_ID\"),\n",
    "    col(\"m.Units_Produced\"),\n",
    "    col(\"m.Defective_Units\"),\n",
    "    col(\"m.Production_Time_min\"),\n",
    "    col(\"s.Power_Consumption_kW\"),\n",
    "    col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "    col(\"ms.StatusName\").alias(\"Machine_Status\"),\n",
    "    col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "    col(\"ac.AlarmDescription\").alias(\"Alarm_Code\")\n",
    ")\n",
    "\n",
    "# Optional: Write to StarRocks\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"✅ Data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3333777-a68e-425d-9fe3-a443508155ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to write to StarRocks: An error occurred while calling o511.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 47.0 failed 4 times, most recent failure: Lost task 1.3 in stage 47.0 (TID 41) (172.18.0.11 executor 0): java.sql.BatchUpdateException: Failed to load data into tablet 400271, because of too many versions, current/limit: 1002/1000. You can reduce the loading job concurrency, or increase loading data batch size. If you are loading data with Routine Load, you can increase FE configs routine_load_task_consume_second and max_routine_load_batch_size,:  be:127.0.0.1\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:816)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:418)\n",
      "\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:708)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.sql.SQLException: Failed to load data into tablet 400271, because of too many versions, current/limit: 1002/1000. You can reduce the loading job concurrency, or increase loading data batch size. If you are loading data with Routine Load, you can increase FE configs routine_load_task_consume_second and max_routine_load_batch_size,:  be:127.0.0.1\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:130)\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:916)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1061)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:795)\n",
      "\t... 16 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:867)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.sql.BatchUpdateException: Failed to load data into tablet 400271, because of too many versions, current/limit: 1002/1000. You can reduce the loading job concurrency, or increase loading data batch size. If you are loading data with Routine Load, you can increase FE configs routine_load_task_consume_second and max_routine_load_batch_size,:  be:127.0.0.1\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:816)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:418)\n",
      "\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:708)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.sql.SQLException: Failed to load data into tablet 400271, because of too many versions, current/limit: 1002/1000. You can reduce the loading job concurrency, or increase loading data batch size. If you are loading data with Routine Load, you can increase FE configs routine_load_task_consume_second and max_routine_load_batch_size,:  be:127.0.0.1\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:130)\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:916)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1061)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:795)\n",
      "\t... 16 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeToMachineMetrics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configs\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Load staging and dimension tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "status_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarm_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# Best MES within ±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 3600)\n",
    "\n",
    "mes_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "mes_best = mes_join.withColumn(\"rn\", row_number().over(mes_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# Best SCADA within ±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 900)\n",
    "\n",
    "scada_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "scada_best = scada_join.withColumn(\"rn\", row_number().over(scada_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\").alias(\"Machine_Status_Name\"),\n",
    "        col(\"s.Alarm_Code\").alias(\"Alarm_Code_Desc\")\n",
    "    )\n",
    "\n",
    "# Join all and enrich with IDs\n",
    "final_df = mes_best.alias(\"m\").join(scada_best.alias(\"s\"),\n",
    "    (col(\"m.Machine_ID\") == col(\"s.Machine_ID\")) & (col(\"m.iot_Timestamp\") == col(\"s.iot_Timestamp\")),\n",
    "    how=\"left\"\n",
    ").join(status_df.alias(\"ms\"), col(\"s.Machine_Status_Name\") == col(\"ms.StatusName\"), \"left\") \\\n",
    " .join(alarm_df.alias(\"ac\"), col(\"s.Alarm_Code_Desc\") == col(\"ac.AlarmDescription\"), \"left\") \\\n",
    " .select(\n",
    "    col(\"m.iot_Timestamp\").alias(\"Timestamp\"),\n",
    "    col(\"m.Machine_ID\"),\n",
    "    col(\"m.Temperature_C\"),\n",
    "    col(\"m.Vibration_mm_s\"),\n",
    "    col(\"m.Pressure_bar\"),\n",
    "    col(\"m.Operator_ID\"),\n",
    "    col(\"m.Units_Produced\"),\n",
    "    col(\"m.Defective_Units\"),\n",
    "    col(\"m.Production_Time_min\"),\n",
    "    col(\"s.Power_Consumption_kW\"),\n",
    "    col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "    col(\"ms.StatusName\").alias(\"Machine_Status\"),\n",
    "    col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "    col(\"ac.AlarmDescription\").alias(\"Alarm_Code\")\n",
    ")\n",
    "\n",
    "# Optional: Write to StarRocks\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"✅ Data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3d9f2-03fd-431c-a06d-d204718c9dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
